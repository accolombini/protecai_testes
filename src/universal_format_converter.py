#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
universal_format_converter.py ‚Äî Conversor Universal para CSV

OBJETIVO: Converter TODOS os formatos (PDF, TXT, XLSX, CSV) para CSV padronizado
para que o pipeline tenha uma √∫nica entrada uniforme.

ARQUITETURA UNIFICADA:
inputs/{pdf,txt,xlsx,csv} ‚Üí [CONVERSOR UNIVERSAL] ‚Üí outputs/csv/ ‚Üí [PIPELINE √öNICO]

Benef√≠cios:
- Pipeline √∫nico e consistente
- F√°cil manuten√ß√£o
- Padroniza√ß√£o de dados
- Facilita an√°lise comparativa

Autor: Sistema ProtecAI
Data: 2025-10-18
"""

from __future__ import annotations
import sys
import argparse
import re
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any
from datetime import datetime

import pandas as pd
from PyPDF2 import PdfReader

# Imports locais
try:
    from .file_registry_manager import FileRegistryManager
except ImportError:
    from file_registry_manager import FileRegistryManager

# ---------------------------
# Configura√ß√£o de diret√≥rios
# ---------------------------

HERE = Path(__file__).resolve()
PROJECT_ROOT = HERE.parents[1]

INPUT_BASE_DIR = PROJECT_ROOT / "inputs"
INPUT_PDF_DIR = INPUT_BASE_DIR / "pdf"
INPUT_TXT_DIR = INPUT_BASE_DIR / "txt"
INPUT_XLSX_DIR = INPUT_BASE_DIR / "xlsx"
INPUT_CSV_DIR = INPUT_BASE_DIR / "csv"

OUTPUT_CSV_DIR = PROJECT_ROOT / "outputs" / "csv"
OUTPUT_LOGS_DIR = PROJECT_ROOT / "outputs" / "logs"

# Garantir diret√≥rios
for directory in [INPUT_PDF_DIR, INPUT_TXT_DIR, INPUT_XLSX_DIR, INPUT_CSV_DIR, 
                  OUTPUT_CSV_DIR, OUTPUT_LOGS_DIR]:
    directory.mkdir(parents=True, exist_ok=True)


class UniversalFormatConverter:
    """
    Conversor universal que padroniza todos os formatos para CSV.
    
    Entrada: PDF, TXT, XLSX, CSV (em inputs/)
    Sa√≠da: CSV padronizado (Code, Description, Value) em outputs/csv/
    """
    
    def __init__(self, verbose: bool = False):
        """
        Inicializa o conversor.
        
        Args:
            verbose: Se True, mostra output detalhado
        """
        self.verbose = verbose
        self.stats = {
            'inicio': datetime.now().isoformat(),
            'arquivos_processados': [],
            'arquivos_ignorados': [],
            'erros': [],
            'formatos_encontrados': {}
        }
        
        # Patterns para parsing de texto
        self.re_micom = re.compile(r"^([0-9A-F]{2}\.[0-9A-F]{2}):\s*([^:]+):\s*(.*)$")
        self.re_easergy_equals = re.compile(r"^([0-9A-F]{4}):\s*([^=:]+)=:\s*(.*)$")
        self.re_easergy_colon = re.compile(r"^([0-9A-F]{4}):\s*([^:]+):\s*(.*)$")
        self.re_structured = re.compile(r"^([^:]+):\s*([^:]+):\s*(.*)$")
        
        if self.verbose:
            print(f"üîß Conversor Universal iniciado")
    
    def log_info(self, msg: str):
        """Log de informa√ß√£o"""
        if self.verbose:
            print(f"‚ÑπÔ∏è  {msg}")
    
    def log_success(self, msg: str):
        """Log de sucesso"""
        print(f"‚úÖ {msg}")
    
    def log_warning(self, msg: str):
        """Log de warning"""
        print(f"‚ö†Ô∏è  {msg}")
    
    def log_error(self, msg: str):
        """Log de erro"""
        print(f"‚ùå {msg}")
    
    def extract_text_from_pdf(self, pdf_path: Path) -> str:
        """Extrai texto completo do PDF"""
        try:
            reader = PdfReader(str(pdf_path))
            parts = []
            for page in reader.pages:
                text = page.extract_text()
                if text:
                    parts.append(text)
            return "\n".join(parts)
        except Exception as e:
            self.log_error(f"Erro ao extrair PDF {pdf_path.name}: {e}")
            return ""
    
    def parse_text_to_dataframe(self, text: str, source_format: str) -> Tuple[pd.DataFrame, str]:
        """
        Converte texto em DataFrame padronizado (Code, Description, Value)
        
        Returns:
            Tupla (DataFrame, formato_detectado)
        """
        rows = []
        detected_format = "unknown"
        
        # Detectar formato do relay
        lowered = text.lower()
        if "micom s1 agile" in lowered or "micom p24" in lowered:
            detected_format = "micom"
        elif "easergy studio" in lowered or "easergy p2" in lowered:
            detected_format = "easergy" 
        elif "generic" in source_format or "txt" in source_format:
            detected_format = "structured_text"
        
        for raw_line in text.splitlines():
            line = raw_line.strip()
            if not line or line.startswith('#') or line.startswith('//'):
                continue
            
            # Tentar patterns MiCOM primeiro
            m = self.re_micom.match(line)
            if m:
                code, desc, val = m.groups()
                rows.append({
                    "Code": code.strip(),
                    "Description": desc.strip(), 
                    "Value": val.strip()
                })
                if detected_format == "unknown":
                    detected_format = "micom"
                continue
            
            # Patterns Easergy
            m = self.re_easergy_equals.match(line)
            if m:
                code, desc, val = m.groups()
                rows.append({
                    "Code": code.strip(),
                    "Description": desc.strip(),
                    "Value": val.strip()
                })
                if detected_format == "unknown":
                    detected_format = "easergy"
                continue
                
            m = self.re_easergy_colon.match(line)
            if m:
                code, desc, val = m.groups()
                rows.append({
                    "Code": code.strip(),
                    "Description": desc.strip(),
                    "Value": val.strip()
                })
                if detected_format == "unknown":
                    detected_format = "easergy"
                continue
            
            # Pattern gen√©rico (3 campos separados por :)
            m = self.re_structured.match(line)
            if m:
                code, desc, val = m.groups()
                rows.append({
                    "Code": code.strip(),
                    "Description": desc.strip(),
                    "Value": val.strip()
                })
                if detected_format == "unknown":
                    detected_format = "structured_text"
        
        df = pd.DataFrame(rows)
        
        # Garantir colunas padronizadas
        for col in ["Code", "Description", "Value"]:
            if col not in df.columns:
                df[col] = ""
        
        return df[["Code", "Description", "Value"]], detected_format
    
    def convert_pdf_to_csv(self, pdf_path: Path) -> Optional[Path]:
        """Converte PDF para CSV padronizado"""
        self.log_info(f"Convertendo PDF: {pdf_path.name}")
        
        # Extrair texto
        text = self.extract_text_from_pdf(pdf_path)
        if not text.strip():
            self.log_warning(f"PDF {pdf_path.name} sem texto extra√≠vel")
            return None
        
        # Fazer parse
        df, detected_format = self.parse_text_to_dataframe(text, "pdf")
        
        if df.empty:
            self.log_warning(f"Nenhum dado estruturado encontrado em {pdf_path.name}")
            return None
        
        # Salvar CSV
        output_path = OUTPUT_CSV_DIR / f"{pdf_path.stem}_params.csv"
        df.to_csv(output_path, index=False, encoding='utf-8')
        
        self.log_success(f"PDF convertido: {pdf_path.name} ‚Üí {output_path.name} ({len(df)} linhas, {detected_format})")
        return output_path
    
    def convert_txt_to_csv(self, txt_path: Path) -> Optional[Path]:
        """Converte TXT para CSV padronizado"""
        self.log_info(f"Convertendo TXT: {txt_path.name}")
        
        try:
            # Tentar diferentes encodings
            text = None
            for encoding in ['utf-8', 'latin-1', 'cp1252']:
                try:
                    with open(txt_path, 'r', encoding=encoding) as f:
                        text = f.read()
                    break
                except UnicodeDecodeError:
                    continue
            
            if text is None:
                self.log_error(f"N√£o foi poss√≠vel ler {txt_path.name} com encodings suportados")
                return None
            
            # Parse
            df, detected_format = self.parse_text_to_dataframe(text, "txt")
            
            if df.empty:
                self.log_warning(f"Nenhum dado estruturado em {txt_path.name}")
                return None
            
            # Salvar CSV
            output_path = OUTPUT_CSV_DIR / f"{txt_path.stem}_params.csv"
            df.to_csv(output_path, index=False, encoding='utf-8')
            
            self.log_success(f"TXT convertido: {txt_path.name} ‚Üí {output_path.name} ({len(df)} linhas, {detected_format})")
            return output_path
            
        except Exception as e:
            self.log_error(f"Erro ao converter TXT {txt_path.name}: {e}")
            return None
    
    def convert_xlsx_to_csv(self, xlsx_path: Path) -> Optional[Path]:
        """Converte XLSX para CSV padronizado"""
        self.log_info(f"Convertendo XLSX: {xlsx_path.name}")
        
        try:
            # Ler Excel (tentar m√∫ltiplas sheets se necess√°rio)
            df = None
            try:
                xls = pd.ExcelFile(xlsx_path)
                
                # Procurar sheet com mais dados
                best_df = None
                max_rows = 0
                
                for sheet_name in xls.sheet_names:
                    temp_df = pd.read_excel(xlsx_path, sheet_name=sheet_name, dtype=str)
                    if len(temp_df) > max_rows and len(temp_df.columns) >= 3:
                        best_df = temp_df
                        max_rows = len(temp_df)
                
                df = best_df if best_df is not None else pd.read_excel(xlsx_path, dtype=str)
                
            except Exception:
                df = pd.read_excel(xlsx_path, dtype=str)
            
            if df is None or df.empty:
                self.log_warning(f"XLSX {xlsx_path.name} vazio ou ileg√≠vel")
                return None
            
            # Mapear colunas para formato padr√£o
            column_mapping = self._map_columns_to_standard(df.columns)
            df = df.rename(columns=column_mapping)
            
            # Garantir colunas padr√£o
            for col in ["Code", "Description", "Value"]:
                if col not in df.columns:
                    df[col] = ""
            
            # Limpar dados
            df = df[["Code", "Description", "Value"]].fillna("")
            df = df[df["Code"].astype(str).str.strip() != ""]  # Remover linhas sem c√≥digo
            
            if df.empty:
                self.log_warning(f"Nenhum dado v√°lido em {xlsx_path.name}")
                return None
            
            # Salvar CSV
            output_path = OUTPUT_CSV_DIR / f"{xlsx_path.stem}_params.csv"
            df.to_csv(output_path, index=False, encoding='utf-8')
            
            self.log_success(f"XLSX convertido: {xlsx_path.name} ‚Üí {output_path.name} ({len(df)} linhas)")
            return output_path
            
        except Exception as e:
            self.log_error(f"Erro ao converter XLSX {xlsx_path.name}: {e}")
            return None
    
    def convert_csv_to_standardized_csv(self, csv_path: Path) -> Optional[Path]:
        """Converte CSV para CSV padronizado (normaliza√ß√£o)"""
        self.log_info(f"Padronizando CSV: {csv_path.name}")
        
        try:
            # Tentar diferentes separadores
            df = None
            for sep in [',', ';', '\t', '|']:
                try:
                    temp_df = pd.read_csv(csv_path, sep=sep, dtype=str, encoding='utf-8')
                    if len(temp_df.columns) >= 3:
                        df = temp_df
                        break
                except:
                    continue
            
            if df is None:
                # Fallback para separador padr√£o
                df = pd.read_csv(csv_path, dtype=str, encoding='utf-8')
            
            if df.empty:
                self.log_warning(f"CSV {csv_path.name} vazio")
                return None
            
            # Mapear colunas
            column_mapping = self._map_columns_to_standard(df.columns)
            df = df.rename(columns=column_mapping)
            
            # Garantir colunas padr√£o
            for col in ["Code", "Description", "Value"]:
                if col not in df.columns:
                    df[col] = ""
            
            # Limpar dados
            df = df[["Code", "Description", "Value"]].fillna("")
            df = df[df["Code"].astype(str).str.strip() != ""]
            
            if df.empty:
                self.log_warning(f"Nenhum dado v√°lido em {csv_path.name}")
                return None
            
            # Salvar CSV padronizado
            output_path = OUTPUT_CSV_DIR / f"{csv_path.stem}_params.csv"
            df.to_csv(output_path, index=False, encoding='utf-8')
            
            self.log_success(f"CSV padronizado: {csv_path.name} ‚Üí {output_path.name} ({len(df)} linhas)")
            return output_path
            
        except Exception as e:
            self.log_error(f"Erro ao padronizar CSV {csv_path.name}: {e}")
            return None
    
    def _map_columns_to_standard(self, columns: List[str]) -> Dict[str, str]:
        """Mapeia colunas encontradas para formato padr√£o (Code, Description, Value)"""
        mapping = {}
        
        for col in columns:
            col_lower = str(col).lower().strip()
            
            # Mapear para Code
            if any(term in col_lower for term in ['code', 'c√≥digo', 'cod', 'id', 'ref']):
                mapping[col] = 'Code'
            
            # Mapear para Description  
            elif any(term in col_lower for term in ['desc', 'description', 'descri√ß√£o', 'name', 'nome', 'parameter']):
                mapping[col] = 'Description'
            
            # Mapear para Value
            elif any(term in col_lower for term in ['value', 'valor', 'val', 'setting', 'config', 'data']):
                mapping[col] = 'Value'
        
        return mapping
    
    def convert_all_formats(self) -> Dict[str, Any]:
        """
        Converte todos os formatos encontrados em inputs/ para CSV padronizado
        
        Returns:
            Dicion√°rio com estat√≠sticas do processamento
        """
        # Buscar arquivos em cada formato
        formatos = {
            'pdf': list(INPUT_PDF_DIR.glob("*.pdf")),
            'txt': list(INPUT_TXT_DIR.glob("*.txt")),
            'xlsx': list(INPUT_XLSX_DIR.glob("*.xlsx")) + list(INPUT_XLSX_DIR.glob("*.xls")),
            'csv': list(INPUT_CSV_DIR.glob("*.csv"))
        }
        
        # Estat√≠sticas
        total_encontrados = sum(len(files) for files in formatos.values())
        total_convertidos = 0
        
        self.stats['formatos_encontrados'] = {fmt: len(files) for fmt, files in formatos.items()}
        
        if total_encontrados == 0:
            self.log_warning("Nenhum arquivo encontrado em inputs/")
            return self.stats
        
        print(f"\nüîç Arquivos encontrados: {self.stats['formatos_encontrados']}")
        print(f"üìã Total: {total_encontrados} arquivo(s)\n")
        
        # Converter PDFs
        for pdf_path in formatos['pdf']:
            output_path = self.convert_pdf_to_csv(pdf_path)
            if output_path:
                self.stats['arquivos_processados'].append({
                    'entrada': str(pdf_path),
                    'saida': str(output_path),
                    'formato': 'pdf'
                })
                total_convertidos += 1
            else:
                self.stats['arquivos_ignorados'].append(str(pdf_path))
        
        # Converter TXTs
        for txt_path in formatos['txt']:
            output_path = self.convert_txt_to_csv(txt_path)
            if output_path:
                self.stats['arquivos_processados'].append({
                    'entrada': str(txt_path),
                    'saida': str(output_path),
                    'formato': 'txt'
                })
                total_convertidos += 1
            else:
                self.stats['arquivos_ignorados'].append(str(txt_path))
        
        # Converter XLSXs
        for xlsx_path in formatos['xlsx']:
            output_path = self.convert_xlsx_to_csv(xlsx_path)
            if output_path:
                self.stats['arquivos_processados'].append({
                    'entrada': str(xlsx_path),
                    'saida': str(output_path),
                    'formato': 'xlsx'
                })
                total_convertidos += 1
            else:
                self.stats['arquivos_ignorados'].append(str(xlsx_path))
        
        # Padronizar CSVs
        for csv_path in formatos['csv']:
            output_path = self.convert_csv_to_standardized_csv(csv_path)
            if output_path:
                self.stats['arquivos_processados'].append({
                    'entrada': str(csv_path),
                    'saida': str(output_path),
                    'formato': 'csv'
                })
                total_convertidos += 1
            else:
                self.stats['arquivos_ignorados'].append(str(csv_path))
        
        # Finalizar estat√≠sticas
        self.stats.update({
            'fim': datetime.now().isoformat(),
            'total_encontrados': total_encontrados,
            'total_convertidos': total_convertidos,
            'total_ignorados': len(self.stats['arquivos_ignorados']),
            'total_erros': len(self.stats['erros'])
        })
        
        return self.stats
    
    def mostrar_resumo(self):
        """Mostra resumo da convers√£o"""
        print("\n" + "="*60)
        print("üìä RESUMO DA CONVERS√ÉO UNIVERSAL")
        print("="*60)
        
        print(f"üìÅ Formatos encontrados: {self.stats['formatos_encontrados']}")
        print(f"‚úÖ Convertidos: {self.stats.get('total_convertidos', 0)}")
        print(f"‚è≠Ô∏è  Ignorados: {self.stats.get('total_ignorados', 0)}")
        print(f"‚ùå Erros: {self.stats.get('total_erros', 0)}")
        
        if self.stats['arquivos_processados']:
            print(f"\nüìã Arquivos convertidos:")
            for item in self.stats['arquivos_processados']:
                entrada = Path(item['entrada']).name
                saida = Path(item['saida']).name
                formato = item['formato'].upper()
                print(f"  ‚Ä¢ {formato}: {entrada} ‚Üí {saida}")
        
        if self.stats['arquivos_ignorados']:
            print(f"\n‚ö†Ô∏è  Arquivos ignorados:")
            for arquivo in self.stats['arquivos_ignorados']:
                print(f"  ‚Ä¢ {Path(arquivo).name}")
        
        if self.stats['erros']:
            print(f"\n‚ùå Erros:")
            for erro in self.stats['erros']:
                print(f"  ‚Ä¢ {erro}")
        
        print(f"\nüìÇ Sa√≠das em: {OUTPUT_CSV_DIR.relative_to(PROJECT_ROOT)}")
        print("="*60)


def build_argparser() -> argparse.ArgumentParser:
    """Constr√≥i o parser de argumentos"""
    parser = argparse.ArgumentParser(
        description="Conversor Universal - Todos os formatos para CSV padronizado",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ARQUITETURA UNIFICADA:
inputs/{pdf,txt,xlsx,csv} ‚Üí [CONVERSOR UNIVERSAL] ‚Üí outputs/csv/ ‚Üí [PIPELINE √öNICO]

Exemplos de uso:

  # Converter todos os formatos encontrados
  python src/universal_format_converter.py
  
  # Modo verbose
  python src/universal_format_converter.py --verbose

Resultado:
  ‚Ä¢ Todos os formatos convertidos para CSV padronizado (Code, Description, Value)
  ‚Ä¢ Pipeline √∫nico a partir de outputs/csv/
  ‚Ä¢ Manuten√ß√£o simplificada
  ‚Ä¢ An√°lise comparativa facilitada
        """
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Output detalhado'
    )
    
    return parser


def main() -> int:
    """Fun√ß√£o principal"""
    args = build_argparser().parse_args()
    
    try:
        # Criar conversor
        converter = UniversalFormatConverter(verbose=args.verbose)
        
        # Executar convers√£o
        stats = converter.convert_all_formats()
        
        # Mostrar resumo
        converter.mostrar_resumo()
        
        # Salvar relat√≥rio
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        relatorio_path = OUTPUT_LOGS_DIR / f"universal_conversion_{timestamp}.json"
        
        import json
        with open(relatorio_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"\nüìÑ Relat√≥rio salvo: {relatorio_path.relative_to(PROJECT_ROOT)}")
        
        # C√≥digo de retorno
        if stats.get('total_erros', 0) > 0:
            return 1
        elif stats.get('total_convertidos', 0) == 0:
            return 2  # Nenhum arquivo convertido
        else:
            return 0  # Sucesso
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Convers√£o interrompida pelo usu√°rio")
        return 130
        
    except Exception as e:
        print(f"\nüí• Erro inesperado: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())