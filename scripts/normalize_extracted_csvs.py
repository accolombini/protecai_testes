"""
NORMALIZA√á√ÉO CORRETA - GERA NORM_CSV E NORM_EXCEL

OBJETIVO: Normalizar CSVs extra√≠dos gerando arquivos padronizados
ENTRADA: outputs/csv/*.csv (gerados pelo extract_parameters_from_glossario.py)
SA√çDA: outputs/norm_csv/*.csv E outputs/norm_excel/*.xlsx

CR√çTICO: VIDAS EM RISCO - Normaliza√ß√£o deve manter 100% dos dados do gloss√°rio
"""

import pandas as pd
from pathlib import Path
import logging
from datetime import datetime
import json
from typing import Dict, List

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('outputs/logs/normalization.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class CSVNormalizer:
    """Normalizador de CSVs seguindo estrutura do gloss√°rio"""
    
    # Colunas OBRIGAT√ìRIAS na sa√≠da
    REQUIRED_COLUMNS = [
        'parameter_code',
        'parameter_name',
        'set_value',
        'unit_of_measure',
        'section',
        'subsection',
        'category',
        'source_file'
    ]
    
    # Colunas ADICIONAIS √∫teis
    ADDITIONAL_COLUMNS = [
        'normalized_value',  # Valor convertido para formato padr√£o
        'is_valid',          # Valida√ß√£o do valor
        'extraction_date'    # Data da extra√ß√£o
    ]
    
    def __init__(self):
        self.stats = {
            'total_files': 0,
            'total_parameters': 0,
            'files_with_errors': [],
            'validation_errors': []
        }
    
    def normalize_csv(self, csv_path: Path) -> pd.DataFrame:
        """Normaliza um arquivo CSV"""
        logger.info(f"Normalizando: {csv_path.name}")
        
        try:
            # Ler CSV
            df = pd.read_csv(csv_path, encoding='utf-8')
            
            # MAPEAR COLUNAS DO CSV EXTRA√çDO PARA O FORMATO NORMALIZADO
            # CSVs extra√≠dos t√™m: Code, Description, Value, is_active (desde corre√ß√£o)
            # Precisamos mapear para: parameter_code, parameter_name, set_value
            
            if 'Code' in df.columns and 'Description' in df.columns and 'Value' in df.columns:
                # Renomear colunas principais
                df = df.rename(columns={
                    'Code': 'parameter_code',
                    'Description': 'parameter_name',
                    'Value': 'set_value'
                })
                
                # Se is_active j√° existe no CSV extra√≠do, usar esse valor
                # Se n√£o existir, considerar v√°lido se tiver valor
                if 'is_active' not in df.columns:
                    df['is_active'] = df['set_value'].apply(self._validate_value)
            
            # Adicionar colunas que n√£o existem nos CSVs extra√≠dos
            if 'unit_of_measure' not in df.columns:
                df['unit_of_measure'] = ''
            if 'section' not in df.columns:
                df['section'] = ''
            if 'subsection' not in df.columns:
                df['subsection'] = ''
            if 'category' not in df.columns:
                df['category'] = ''
            if 'source_file' not in df.columns:
                df['source_file'] = csv_path.name
            
            # Adicionar colunas adicionais
            df['normalized_value'] = df['set_value'].apply(self._normalize_value)
            df['is_valid'] = df['is_active']  # is_valid √© IGUAL a is_active (n√£o mais baseado em valor n√£o-vazio)
            df['extraction_date'] = datetime.now().isoformat()
            
            # Reordenar colunas
            all_columns = self.REQUIRED_COLUMNS + self.ADDITIONAL_COLUMNS
            df = df[all_columns]
            
            # Validar dados
            self._validate_dataframe(df, csv_path.name)
            
            logger.info(f"  ‚úì {len(df)} par√¢metros normalizados")
            return df
            
        except Exception as e:
            logger.error(f"  ‚ùå Erro ao normalizar {csv_path.name}: {e}")
            self.stats['files_with_errors'].append(csv_path.name)
            raise
    
    def _normalize_value(self, value) -> str:
        """Normaliza valor para formato padr√£o"""
        if pd.isna(value):
            return ''
        
        value_str = str(value).strip()
        
        # Remover caracteres especiais extras
        value_str = value_str.replace('\r', ' ').replace('\n', ' ')
        value_str = ' '.join(value_str.split())  # Normalizar espa√ßos
        
        return value_str
    
    def _validate_value(self, value) -> bool:
        """Valida se o valor est√° presente e n√£o vazio"""
        if pd.isna(value):
            return False
        
        value_str = str(value).strip()
        return len(value_str) > 0
    
    def _validate_dataframe(self, df: pd.DataFrame, filename: str):
        """Valida DataFrame normalizado"""
        # Verificar par√¢metros sem c√≥digo
        missing_code = df[df['parameter_code'].isna() | (df['parameter_code'] == '')]
        if len(missing_code) > 0:
            self.stats['validation_errors'].append({
                'file': filename,
                'error': f'{len(missing_code)} par√¢metros sem c√≥digo'
            })
        
        # Verificar par√¢metros sem nome
        missing_name = df[df['parameter_name'].isna() | (df['parameter_name'] == '')]
        if len(missing_name) > 0:
            self.stats['validation_errors'].append({
                'file': filename,
                'error': f'{len(missing_name)} par√¢metros sem nome'
            })
        
        # Verificar par√¢metros sem valor
        missing_value = df[df['is_valid'] == False]
        if len(missing_value) > 0:
            logger.warning(f"  ‚ö†Ô∏è  {len(missing_value)} par√¢metros sem valor v√°lido")
    
    def save_normalized(self, df: pd.DataFrame, original_filename: str):
        """Salva arquivo normalizado em CSV e Excel"""
        stem = Path(original_filename).stem
        
        # Salvar CSV normalizado
        csv_output = Path("outputs/norm_csv") / f"{stem}.csv"
        csv_output.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(csv_output, index=False, encoding='utf-8')
        logger.info(f"  üíæ CSV: {csv_output.name}")
        
        # Salvar Excel normalizado
        excel_output = Path("outputs/norm_excel") / f"{stem}.xlsx"
        excel_output.parent.mkdir(parents=True, exist_ok=True)
        
        with pd.ExcelWriter(excel_output, engine='openpyxl') as writer:
            # Aba principal com dados
            df.to_excel(writer, sheet_name='Parameters', index=False)
            
            # Aba com estat√≠sticas
            stats_df = pd.DataFrame([{
                'Total Parameters': len(df),
                'Valid Values': df['is_valid'].sum(),
                'Missing Values': (~df['is_valid']).sum(),
                'Unique Sections': df['section'].nunique(),
                'Unique Subsections': df['subsection'].nunique(),
                'Source File': original_filename,
                'Normalized Date': datetime.now().isoformat()
            }])
            stats_df.to_excel(writer, sheet_name='Statistics', index=False)
            
            # Aba com se√ß√µes
            if not df['section'].isna().all():
                sections = df.groupby('section').size().reset_index(name='count')
                sections.to_excel(writer, sheet_name='Sections', index=False)
        
        logger.info(f"  üíæ Excel: {excel_output.name}")
    
    def process_all_csvs(self):
        """Processa TODOS os CSVs de outputs/csv"""
        logger.info("="*80)
        logger.info("INICIANDO NORMALIZA√á√ÉO COMPLETA")
        logger.info("="*80)
        
        csv_dir = Path("outputs/csv")
        if not csv_dir.exists():
            logger.error(f"‚ùå Diret√≥rio n√£o encontrado: {csv_dir}")
            return
        
        # IMPORTANTE: Normalizar APENAS arquivos _params.csv (n√£o _active_setup.csv)
        # _active_setup.csv s√£o arquivos auxiliares com par√¢metros ativos (feature √∫til)
        # mas n√£o devem ser normalizados nem importados para o banco
        csv_files = sorted(csv_dir.glob("*_params.csv"))
        total_files = len(csv_files)
        
        logger.info(f"üìÅ Encontrados {total_files} arquivos _params.csv para normalizar")
        logger.info(f"   (Ignorando arquivos _active_setup.csv - s√£o auxiliares)")
        
        for idx, csv_file in enumerate(csv_files, 1):
            logger.info(f"\n[{idx}/{total_files}] Processando {csv_file.name}")
            
            try:
                # Normalizar
                df_normalized = self.normalize_csv(csv_file)
                
                # Salvar
                self.save_normalized(df_normalized, csv_file.name)
                
                # Atualizar estat√≠sticas
                self.stats['total_files'] += 1
                self.stats['total_parameters'] += len(df_normalized)
                
            except Exception as e:
                logger.error(f"‚ùå Falha ao processar {csv_file.name}: {e}")
        
        # Relat√≥rio final
        self._generate_final_report()
    
    def _generate_final_report(self):
        """Gera relat√≥rio final da normaliza√ß√£o"""
        logger.info("="*80)
        logger.info("‚úÖ NORMALIZA√á√ÉO CONCLU√çDA")
        logger.info(f"   üìÅ Arquivos processados: {self.stats['total_files']}")
        logger.info(f"   üìä Par√¢metros normalizados: {self.stats['total_parameters']}")
        
        if self.stats['files_with_errors']:
            logger.warning(f"   ‚ö†Ô∏è  Arquivos com erros: {len(self.stats['files_with_errors'])}")
            for filename in self.stats['files_with_errors']:
                logger.warning(f"      - {filename}")
        
        if self.stats['validation_errors']:
            logger.warning(f"   ‚ö†Ô∏è  Erros de valida√ß√£o: {len(self.stats['validation_errors'])}")
            for error in self.stats['validation_errors']:
                logger.warning(f"      - {error['file']}: {error['error']}")
        
        logger.info("="*80)
        
        # Salvar relat√≥rio JSON
        report_path = Path("outputs/logs/normalization_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìã Relat√≥rio salvo: {report_path}")
        
        # Verificar coverage
        self._check_coverage()
    
    def _check_coverage(self):
        """Verifica cobertura dos arquivos normalizados"""
        csv_count = len(list(Path("outputs/csv").glob("*.csv")))
        norm_csv_count = len(list(Path("outputs/norm_csv").glob("*.csv")))
        norm_excel_count = len(list(Path("outputs/norm_excel").glob("*.xlsx")))
        
        logger.info("\nüìä COBERTURA:")
        logger.info(f"   CSVs originais: {csv_count}")
        logger.info(f"   CSVs normalizados: {norm_csv_count}")
        logger.info(f"   Excel normalizados: {norm_excel_count}")
        
        if norm_csv_count == csv_count and norm_excel_count == csv_count:
            logger.info("   ‚úÖ 100% de cobertura!")
        else:
            logger.warning("   ‚ö†Ô∏è  Cobertura incompleta!")


def main():
    """Execu√ß√£o principal"""
    try:
        normalizer = CSVNormalizer()
        normalizer.process_all_csvs()
        
    except Exception as e:
        logger.error(f"ERRO CR√çTICO: {e}")
        raise


if __name__ == "__main__":
    main()
